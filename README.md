# ITMS 478 Research Project — Adversarial Attacks on Machine Learning in Network Security
This project explores how adversarial attacks can degrade the performance of machine learning models in network security applications and tests defense strategies that improve model robustness.

Objectives

- Reconstruct a baseline intrusion-detection model using the NSL-KDD (or CICIDS2017) dataset.
- Implement adversarial attacks (FGSM and PGD) to test model robustness.
- Evaluate how these attacks affect accuracy, recall, and precision.
- Propose and test defenses such as:
Adversarial training
Noise injection
Adversarial sample detection using autoencoders

Research Context

- Machine learning-based cybersecurity models, such as intrusion detection or malware detection systems, are vulnerable to evasion attacks that slightly modify inputs to cause misclassification.
- This project aims to measure that vulnerability and demonstrate strategies to make such models more resilient.

Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1-Score
- ROC-AUC
- Performance degradation vs. ε (attack strength)

Loom Recording: https://www.loom.com/share/79c9721caf4e4f5096ecd2d5f18a6b70?sid=8186b938-0e8b-4446-93f9-efbe86daa9a6

