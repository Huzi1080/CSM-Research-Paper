# Adversarial Robustness in Machine Learning-Based Network Security

This research project investigates how adversarial attacks degrade the performance of machine-learning models used in network security applications and explores defense strategies that improve model robustness.

---

## Overview

Machine-learning models are increasingly used in cybersecurity systems such as intrusion detection, malware classification, and phishing detection.  
However, these models are vulnerable to **adversarial attacks**â€”small, carefully crafted input perturbations that cause misclassification.

This project replicates and extends prior research on adversarial robustness to:
1. Reconstruct a baseline intrusion-detection model.
2. Generate and evaluate adversarial attacks.
3. Implement and test defense mechanisms that harden ML models against such attacks.

---

##  Research Objectives

1. Reconstruct a baseline intrusion-detection model using **NSL-KDD** or **CICIDS2017** dataset.  
2. Implement **Fast Gradient Sign Method (FGSM)** and **Projected Gradient Descent (PGD)** attacks to assess robustness.  
3. Evaluate model performance degradation in terms of:
   - Accuracy  
   - Precision  
   - Recall  
   - F1-Score  
   - ROC-AUC  
4. Propose and test defense strategies:
   - Adversarial training  
   - Noise injection  
   - Adversarial-sample detection using autoencoders  

---

## ðŸ“‚ Repository Structure



Loom Recording: https://www.loom.com/share/79c9721caf4e4f5096ecd2d5f18a6b70?sid=8186b938-0e8b-4446-93f9-efbe86daa9a6

